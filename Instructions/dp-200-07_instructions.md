---
lab:
    title: 'Azure Data Factory でのデータ移動のオーケストレーション'
    module: 'モジュール 7: Azure Data Factory でのデータ移動のオーケストレーション'
---

# DP 200 - データ プラットフォーム ソリューションの実装
# ラボ 7 - Azure Data Factory でのデータ移動のオーケストレーション

**所要時間**: 45 分

**前提条件**: このラボのケース スタディは既に確認していることを前提としています。内容とラボを前提としています。モジュール 1：Azure for the Data Engineer のコースとラボも完了済みであることを前提としています。

**ラボ ファイル**: このラボのファイルは、_Allfiles\Labfiles\Starter\DP-200.7_ のフォルダーにあります。

## ラボの概要

このモジュールでは、Azure Data Factory を使用して、さまざまなデータ プラットフォーム テクノロジーからのデータ移動を統合する方法について説明します。受講者は、テクノロジーの機能を説明できるようになり、またSQL Database からデータを取り込み、データを SQL Data Warehouse に読み込むエンド ツー エンドのデータ パイプラインをセットアップする方法を学びます。また、コンピューティング リソースの呼び出し方法をデモンストレーションすることもできるようになります。

## ラボの目的
  
このモジュールを修了すると、次のことができるようになります:

1. データ ストリームとイベント処理について説明する
1. Event Hubs を使用したデータ インジェストが実行できる
1. Stream Analytics ジョブを使用してデータを処理できる

## シナリオ
  
情報サービス部門は、Azure SQL Data Warehouse への Data Warehouse の初期配置を実行した後、このプロセスを自動化したいと考えています。あなたは、Azure SQL Database からのデータの移動を自動化できるソリューションを開発するうえで、情報サービス部門をサポートするよう求められました。

ソリューションは、Azure SQL Data Warehouse で同じ名前のディメンション テーブルとして機能する [SalesLT].[ProductCategory] および [SalesLT].[ProductDescription] トランザクション テーブルの完全なコピーを実行できる必要があります。さらに、このソリューションは、Azure Data Factory をデータ移動のオーケストレーターとして使用する超並列処理　(MPP) システムへの読み込みのベスト プラクティスにも従う必要があります。

さらに、データ サイエンティストは、Azure Data Factory から Azure Databricks を呼び出すことができるかどうかを確認するよう求めています。そのために、あなたには Azure Databricks をコンピューティング リソースとして呼び出す Data Factory の概念パイプラインの簡単な証明を作成してもらいます。

このラボを完了すると:

1. Azure Data Factory のしくみを説明する
1. Azure Data Factory のコンポーネント
1. Azure Data Factory と Databricks

> **重要**: このラボを進めるにつれ、プロビジョニングまたは構成タスクで発生した問題を書き留め、_\Labfiles\DP-200-Issues-Docx_ にあるドキュメントの表に記録してください。ラボ番号を文書化し、テクノロジーを書き留め、問題とその解決を説明してください。このドキュメントは、後のモジュールで参照できるように保存します。

## エクササイズ 1: Azure Data Factory のしくみを説明する

所要時間: 15 分

グループ エクササイズ
  
このエクササイズの主なタスク:

1. ケース スタディとシナリオから、AdventureWorks のデータ ストリーム インジェスト技術と、データ エンジニアとして実行する高レベルのタスクを特定して、ソーシャル メディア分析の要件を完了します。

1. インストラクターは、調査結果についてグループと話し合います。

### タスク 1: AdventureWorks のデータ要件と構造を特定する。

1. ラボの仮想マシンから **Microsoft Word** を起動し、**Allfiles\Labfiles\Starter\DP-200.6** フォルダーからファイル  **DP-200-Lab06-Ex01.docx** を開きます。

1. ケース スタディ ドキュメント内でグループが特定したデータ要件とデータ構造について、グループで **10 分間** 話し合い、書き出してもらいます。

### タスク 2: 調査結果についてインストラクターと話し合う

1. インストラクターは、調査結果について話し合うためにグループ作業を中断させます。

> **結果**: このエクササイズを完了すると、データ ストリーミング インジェストと、ソーシャル メディア分析の要件を完了するためにデータ エンジニアとして実行する高レベルのタスクのテーブルを示す Microsoft Word ドキュメントが作成されます。

## エクササイズ 2: Azure Data Factory コンポーネント
  
所要時間: 15 分

個別エクササイズ
  
このエクササイズの主なタスク:

1. データ ファクトリ インスタンスを作成する。

1. 入力リンクト サービスを作成する

1. 入力データセットを定義する

1. 出力リンクト サービスを作成する

1. 出力データセットを定義する

1. 設定を確定し、SQL Data Warehouse を最適化する

1. パイプラインの実行を監視する

1. Azure Data Factory コンポーネントを確認する

1. データ出力を確認する

### タスク 1: データ ファクトリ インスタンスを作成する。

1. Microsoft Edge で、[Azure ポータル] タブに移動し、[**+ Create a resource**] (+ リソースを作成) を選択し、「**factory**」と入力し、 結果の検索から [**データ ファクトリ**] を選択します。次に、[**作成**] を選択します。

1. 次のオプションを使用して新規データ ファクトリを作成し、[**作成**] をクリックします。
    - **名前**: xx-data-factory (xx は自分のイニシャル)
    - **サブスクリプション**: 自分のサブスクリプション
    - **リソース グループ**: awrgstudxx
    - **バージョン**: V2
    - **場所**: ユーザーに近い場所を選択します。
    - 他のオプションを既定のままにします

    > **注記**: Data Factory の作成には約 1 分かかります。

### タスク 2: 入力リンクト サービスを作成する

1. Azure portal で、Azure Data Factory のインストールが正常に完了したことを確認するメッセージが表示されたら、[**リソースに移動**] をクリックします。

1. **xx-data-factory** 画面で、[**Author & Monitor**] (作成者とモニター) をクリックします。新しいタブが開くので、Azure Data Factory ソリューションを作成します。

1. [**Get started**](開始) ページで、[**Copy Data**] (データのコピー) タイルを選択してデータ コピー ツールを起動します。

1. [**プロパティ**] ページで、タスク名フィールドに **CopyFromSQLToSQLDW** を指定し、[次へ] を選択します。

1. [**ソース データ ストア**] ページで、次の手順を実行し、[**次へ**] をクリックします。
    - [**+ Create new connection**] (+ 新しい接続の作成) をクリックします。
    - ギャラリーから [**Azure SQL Database**] を選択し、[**続行**] を選択します。
    - [**New Linked Service (Azure SQL Database)**] ページで、ドロップダウン リストからサーバー名 **sqlservicexx** とデータベース名 **DeptDatabasesxx** を選択し、ユーザー名とパスワードを指定します。[**Test Connection**] (テスト接続) をクリックして設定を検証し、[**完了**] をクリックします。

### タスク 3: 入力データセットを定義する

1. データ コピー ウィザードの [データをコピーするテーブルを選択するか、カスタム クエリを使用する] で、[**Existing Tables**] (既存のテーブル) の下の [SalesLT].[ProductCategory] と [SalesLT].[ProductDescription] の横にあるチェックボックスをクリックし、[**次へ**] をクリックします。

### タスク 4: 出力リンクト サービスを作成する

1. [**デスティネーション データ ストア**] ページで、次の手順を実行し、[**次へ**] をクリックします。
    - [**+ Create new connection**] (+ 新しい接続の作成) をクリックします。
    - ギャラリーから [**Azure SQL Data Warehouse**] を選択し、[**続行**] を選択します。
    - [**New Linked Service (Azure SQL Data Warehouse)**] ページで、ドロップダウン リストからサーバー名 **sqlservicexx** とデータベース名 **DWDB** を選択し、ユーザー名とパスワードを指定します。[**Test Connection**] (テスト接続) をクリックして設定を検証し、[**完了**] をクリックします。

### タスク 5: 出力データセットを定義する

1. データ コピー ウィザードの [テーブル マッピング] の [**ソース**] の下で、[SalesLT].[ProductCategory] と [SalesLT].[ProductDescription] の横にあるチェックボックスがオンになっており、デスティネーションが同じ名前であることを確認します。[**次へ**] をクリックします。

1. データ コピー ウィザードの [Column mappings] (列マッピング) で、ソースとデスティネーションの間の列のコピーである **列マッピング** を読み取ります。[**次へ**] をクリックします。

### タスク 6: 設定を確定し、SQL Data Warehouse を最適化する

1. データ コピー ウィザードの [設定] の [**Performance settings**] (パフォーマンス設定) の下で、[**Enable staging**] (ステージングを有効にする) の横にあるチェックボックスがオンであることを確認します。

1. [アカウント リンクト サービスのステージング] の横にある [**+ 新規**] をクリックし、[新しいリンクト サービス] ページでストレージ アカウント **awsastudxx** を選択し、[**完了**] を選択します。

1. [**Advanced settings**] (詳細設定) セクションで、[**Allow PolyBase**] (PolyBase を許可する) がオンであることを確認します。 

1. [**詳細設定**] セクションで、[**Use type default**] (タイプ既定の使用) オプションの選択を解除し、[**次へ**] をクリックします。

1. [**概要**] ページで、設定を確認し、[**次へ**] を選択します。 

### タスク 7: パイプラインの実行を監視する

1. デプロイ ページで、[監視] を選択してパイプライン タスクを **監視** します。

1. 左側の [監視] タブが自動的に選択されることを確認してください。[**アクション**] の列には、 **アクティビティの実行の詳細を表示** したり、パイプラインを再実行したりするためのリンクが含まれています。

1. パイプラインの実行に関連付けられているアクティビティの実行を表示するには、[アクション] 列で [**アクティビティの実行を表示**] リンクを選択します。パイプライン実行ビューに戻るには、上部にある [**パイプライン**] リンクを選択します。[**更新**] を選択してリストを更新します。

1. 各コピー アクティビティの実行の詳細を監視するには、アクティビティ監視ビューの [アクション] の下の [**詳細**] リンクを選択します。ソースからシンクにコピーされたデータの量、データ スループット、期間付きの実行ステップ、使用された構成などの詳細を監視できます。

### タスク 8: Azure Data Factory コンポーネントを確認する

1. [**Author & Monitor**] (作成者とモニター) スクリーンで、[**Author**] (作成者) をクリックします。

1. 実行したばかりのコピー ウィザードで定義されている **1 パイプライン** と **2 データセット** が存在することを [**ファクトリ リソース**] で確認します。

1. Microsoft Edge の Azure Data Factory タブを閉じる

### タスク 9: データ出力を確認する

1. Windows デスクトップで [**スタート**] をクリックし、**「SQL Server」** と入力し、[**Microsoft SQL Server Management Studio 17**] をクリックします。

1. [**Connect to Server**] (サーバーに接続) ダイアログ ボックスで、次の詳細情報を入力します。
    - サーバー名: **sqlservicexx.database.windows.net**
    - 認証: **SQL Server の認証**
    - ユーザー名: **xxsqladmin**
    - パスワード: **P@ssw0rd**

1. [**Connect to Server**] (サーバーに接続) ダイアログ ボックスで、[**Connect**] (接続) をクリックします。

1. データベース **DWDB ** を展開してから **テーブル** を展開し、[SalesLT].[ProductCategory] と [SalesLT].[ProductDescription] が存在することを確認します。


1. **Microsoft SQL Server Management Studio** を閉じます

> **結果**: このエクササイズを完了すると、Azure Data Factory コンポーネントを作成して、ウィザードを使用して Azure SQL Database から Azure SQL Data Warehouse にデータを移動することができます。使用するコンポーネントと、データが Data Warehouse に読み込まれていることを確認することができます。

## エクササイズ 3: Azure Data Factory と Databricks
  
所要時間: 15 分

個別エクササイズ
  
このエクササイズの主なタスク:

1. Databricks アクセス トークンを生成する。

1. Databricks Notebook を生成する

1. リンクト サービスを作成する

1. Databricks Notebook アクティビティを使用するパイプラインを作成する。

1. パイプライン実行をトリガーする。

### タスク 1: Databricks アクセス トークンを生成する。

1. Azure portal で、**[リソース グループ]**、**[awrgstudxx]**、**[awdbwsstudxx]** の順にクリックします (xx は名前のイニシャル)。

1. [**Launch Workspace**] (ワークスペースの起動) をクリックします。

1. Databricks ワークスペースの右上隅にあるユーザー **プロファイル アイコン** をクリックします。

1. [**User Settings**] (ユーザー設定) をクリックします。

1. [アクセス トークン] タブに移動し、[**Generate New Token**] (新しいトークンの生成)をクリックします。

1. 「ADF 統合用」の **コメント** に説明を入力し、**有効期間** を 10 日間に設定し、[**生成**] をクリックします。

1. 生成されたトークンをコピーし、メモ帳 に保存し、[**完了**] をクリックします。

### タスク 2: Databricks Notebook を生成する

1. 画面の左側にある [**ワークスペース**] アイコンをクリックして [ワークスペース] という単語の横にある矢印をクリックし、[**作成**] をクリックして [**フォルダー**] をクリックします。フォルダーに **adftutorial** と名前を付け、[**Create Folder**] (フォルダーの作成) をクリックします。adftutorial フォルダーがワークスペースに表示されます。

1. adftutorial の横にあるドロップダウン矢印、**[作成]**、**[Notebook]**、の順にクリックします。

1. [Notebook の作成] ダイアログ ボックスで「**mynotebook**」の名前を入力し、言語が **Python** を指定していることを確認して、[**作成**] をクリックします。「mynotebook」がタイトルのノートブックが表示されます/

1. 新規作成したノートブック 「mynotebook"」では、次のコードを追加します。

    ```Python
    # パラメーターを活用するためのウィジェットを作成し、パラメーターを印刷する

    dbutils.widgets.text("input", "","")
    dbutils.widgets.get("input")
    y = getArgument("input")
    print ("Param -\'input':")
    印刷 (y)
    ```

    > **注意**: ノートブックのパスは **/adftutorial/mynotebook** です。

### タスク 3: リンクト サービスを作成する

1. Microsoft Edge で、Azure portal のポータル タブをクリックし、Azure Data Factory に戻ります。

1. **xx-data-factory** 画面で、[**Author & Monitor**] (作成者とモニター) をクリックします。新しいタブが開くので、Azure Data Factory ソリューションを作成します。

1. 画面の左側にある [**Author**] (作成者) アイコンをクリックします。Data Factory デザイナーが開きます。

1. 画面の下部にある [**接続**] をクリックし、[**+ 新規**] をクリックします。

1. [**新しいリンクト サービス**] 画面の上部にある [**Compute**] (計算) をクリックして [**Azure Databricks**] をクリックし、[**続行**] をクリックします。

1. [**新しいリンクト サービス (Azure Databricks)**] 画面で次の詳細を入力し、[**完了**] をクリックします。
    - **名前**: xx_dbls(xx は自分のイニシャル)
    - **Databricks ワークスペース**:awdbwsstudxx (xx は自分のイニシャル)
    - **クラスターの選択**: 既存を使用します
    - **ドメイン/リージョン**: 入力する必要があります
    - **アクセス トークン**: メモ帳 からアクセス トークンをコピーし、このフィールドに貼り付けます。
    - **既存のクラスターから選択する**: awdbclstudxx (xx は自分のイニシャル)
    - 他のオプションを既定のままにします

    > **注記**: 終了をクリックすると、xx_dbls が作成され、 [**作成者とモニター**] 画面に戻ります。その他のリンクト サービスは前回のエクササイズで作成されています。

### タスク 5: Databricks ノートブック アクティビティを使用するパイプラインを作成する。

1. 画面の左側にある [ファクトリ リソース] の下で、**+** アイコンをクリックし、[**パイプライン**] をクリックします。これにより、パイプライン デザイナーのタブが開きます。

1. パイプライン デザイナーの下部にある [パラメーター] タブをクリックし、**[+ 新規]** をクリックします。

1. **name** の名前と **string** のタイプを有するパラメーターを作成します。

1. [**アクティビティ**] メニューで、[**Databricks**] を展開します。

1. **Notebook** をクリックしてキャンバスにドラッグします。

1. 下部にある **[Notebook1]** ウィンドウのプロパティで、次の手順を実行します。
    - [**Azure Databricks**] タブに移動します。
    - 前の手順で作成した **xx_dbls** を選択します。

    - [**設定**] タブに移動し、Notebook パスに **/adftutorial/mynotebook** を入力します。
    - [**Base Parameters**] (基本パラメーター) を展開し、[**+ 新規**] をクリックします。
    - **input** の名前と **@pipeline()parameters.name** の値を有するパラメーターを作成します。

1. **Notebook1** で、[テンプレートとして保存] の横にある [**検証**] をクリックします。画面の右側にウィンドウが表示され、「パイプラインが検証されました。
エラーは見つかりませんでした。」と表示されます。>> をクリックしてウィンドウを閉じます。

1. リンクト サービスとパイプラインを公開するには、[**Publish All**] (すべて公開) をクリックします。

    > **注記**: 展開が成功したことを知らせるメッセージが表示されます。

### タスク 6: パイプライン実行をトリガーする。

1. **Notebook1** で、[**Add trigger**] (トリガーの追加) をクリックし、[デバッグ] ボタンの横にある [**Trigger Now**] (トリガーの実行) をクリックします。

1. [**Pipeline Run**] (パイプライン実行) ダイアログ ボックスで、名前パラメーターが要求されます。ここで、パラメーターとして **/path/filename** を使用します。[完了] をクリックします。キャンバスの Notebook1 アクティビティの上に赤い円が表示されます。

### タスク 7: パイプラインを監視する

1. 画面の左側にある [**監視**] タブをクリックします。パイプラインの実行が表示されたことを確認します。ノートブックが実行される Databricks ジョブ クラスターの作成の所要時間は約 5 ~ 8 分です。

1. [**更新**] を定期的に実行して、パイプライン実行の状態を確認します。

1. パイプラインの実行に関連付けられたアクティビティの実行を表示するには、[**アクション**] 列で [**アクティビティの実行の表示**] を選択します。

### タスク 8: 出力を確認する

1. Microsoft Edge で、**mynotebook - Databricks** タブをクリックします。 

1. **Azure Databricks** のワークスペースで [**クラスター**] をクリックすると、ジョブのステータスが実行未確定、実行中、または終了済みとして表示されます。

1. **awdbclstudxx** クラスターをクリックし、**イベント ログ** をクリックしてアクティビティを表示します。

    > **注記**: パイプラインの実行をトリガーした時点で **開始** するイベント タイプが表示されます。